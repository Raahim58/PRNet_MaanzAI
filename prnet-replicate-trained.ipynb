{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":3324348,"datasetId":576013,"databundleVersionId":3375308,"isSourceIdPinned":false},{"sourceType":"kernelVersion","sourceId":251697050}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PRNet: Progressive Resolution based Network for Radiograph based disease classification\n\n- Reproduce the architecture and results from the PRNet paper\n  - paper link: file:///Users/Raahim/Documents/LUMS/Sophomore/Sophomore%20Summer%20semester%202025/research/maanz-ai%20internship/PRNet_Progressive_Resolution_based_Network_for_Radiograph_based_disease_classification.pdf (attach official link or github link)\n  - dataset: https://cxr-covid19.grand-challenge.org/\n  - backup dataset: https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database","metadata":{"_uuid":"ce5ba1da-6a55-4432-b3d4-2e2ed97817ed","_cell_guid":"4351e4c1-f7f5-4d42-8449-6b7573f18ac3","trusted":true,"collapsed":false,"id":"8LlHthoA2TsN","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 0. Starter code","metadata":{"_uuid":"09cc204d-751a-408f-99d4-366e9783ddc0","_cell_guid":"47351b35-ccca-423c-9383-e12d6eb99407","trusted":true,"collapsed":false,"id":"TazHySrX22Zo","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os, random, time\nfrom glob import glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport time\nfrom tqdm.auto import tqdm\nimport shutil\nimport warnings\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom torchvision import transforms, utils\nfrom torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n\nimport timm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2","metadata":{"_uuid":"82710a13-07f2-48a7-9fa7-65d99167f06e","_cell_guid":"3d7a3eed-2987-43be-90d6-167bc0926cb0","trusted":true,"collapsed":false,"id":"hVPVC9bvWdfZ","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fix random seeds for reproducibility\ndef set_global_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_global_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"_uuid":"795387bc-d6e4-4f52-9899-c712a6967de3","_cell_guid":"1337186a-efd8-4722-9aeb-df622d3fc9ad","trusted":true,"collapsed":false,"id":"O2FuB3-D2SX0","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Readying code: Dataset, transforms and visualizations","metadata":{"_uuid":"407e1060-9b52-42c4-90da-e945e23c6696","_cell_guid":"23a2523c-f3b9-48e1-a134-143805c32317","trusted":true,"collapsed":false,"id":"UbfHBBga27il","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"tawsifurrahman/covid19-radiography-database\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"_uuid":"255a2065-b828-463e-bf46-4a8c9ad8caaa","_cell_guid":"a42ff2cf-70d8-4d38-801c-d1e6057bc8c9","trusted":true,"collapsed":false,"id":"zOGqMEeLbf9T","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# List folders/files inside the dataset root directory\nfor root, dirs, files in os.walk(path):\n    print(f\"\\nInspecting folder: {root}\")\n    print(\"Subdirectories:\", dirs)\n    print(\"Files:\", files)\n    # break  # Only list top-level directories (remove this to go deeper)","metadata":{"_uuid":"2ce3e5c2-fcd7-41e2-a1f5-4fb90f933326","_cell_guid":"48b4d2df-5f26-4692-a289-c02c02a41b7a","trusted":true,"collapsed":false,"id":"Grl9JU40cWuU","_kg_hide-output":true,"scrolled":true,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# set the base paths\nbase_dataset_path = os.path.join(path, \"COVID-19_Radiography_Dataset\")\noutput_base = \"data\"\nsplits = ['train', 'val', 'test']\nsplit_ratio = [0.8, 0.1, 0.1]  # 70% train, 15% val, 15% test\nclasses = ['COVID', 'Normal', 'Viral Pneumonia', 'Lung_Opacity']\n\n# create output directories\nfor split in splits:\n    for class_name in classes:\n        os.makedirs(os.path.join(output_base, split, class_name), exist_ok=True)\n\n# function to split and copy images\nfor class_name in classes:\n    source_dir = os.path.join(base_dataset_path, class_name, \"images\")\n    all_images = os.listdir(source_dir)\n    random.shuffle(all_images)\n    n_total = len(all_images)\n    n_train = int(split_ratio[0] * n_total)\n    n_val = int(split_ratio[1] * n_total)\n    train_images = all_images[:n_train]\n    val_images = all_images[n_train:n_train + n_val]\n    test_images = all_images[n_train + n_val:]\n\n    def copy_images(image_list, split_name):\n        for image in tqdm(image_list, desc=f\"{split_name} - {class_name}\"):\n            src = os.path.join(source_dir, image)\n            dst = os.path.join(output_base, split_name, class_name, image)\n            shutil.copyfile(src, dst)\n    copy_images(train_images, \"train\")\n    copy_images(val_images, \"val\")\n    copy_images(test_images, \"test\")","metadata":{"_uuid":"3ec075ef-42ed-4980-9e91-3cfa60cadcec","_cell_guid":"61af348d-63a1-483e-9c78-4fdf16441617","trusted":true,"collapsed":false,"id":"3iMWunBOc7OM","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# loading dataset\n# class ChestXRayDataset(Dataset):\n#   def __init__(self, root_dir: str, class_names: list, transform=None):\n#     self.image_paths = []\n#     self.labels = []\n#     for idx, class_name in enumerate(class_names):\n#       class_dir = os.path.join(root_dir, class_name)\n#       files = glob(class_dir) # get all images of each class\n#       self.image_paths += files\n#       self.labels += [idx] * len(files) # appends label for each class image found\n#     self.transform = transform\n\n#   def __len__(self):\n#     return len(self.image_paths)\n\n#   def __getitem__(self, index):\n#     image_path = self.image_paths[index]\n#     image = Image.open(image_path).convert(\"RGB\")\n#     label = self.labels[index]\n#     if self.transform:\n#       image = self.transform(image)\n#     return image, label\nclass ChestXRayDataset(Dataset):\n    def __init__(self, data_dir, class_names, transform=None):\n        # self.data_dir = data_dir\n        # self.class_names = class_names\n        # self.transform = transform\n        # self.image_paths = []\n        # self.labels = []\n        # for class_name in class_names:\n        #     class_dir = os.path.join(data_dir, class_name)\n        #     for fname in os.listdir(class_dir):\n        #         fpath = os.path.join(class_dir, fname)\n        #         if os.path.isfile(fpath) and fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n        #             self.image_paths.append((fpath, class_name))\n        #             self.labels.append()\n        self.image_paths = []\n        self.labels = []\n        for idx, class_name in enumerate(class_names):\n            class_dir = os.path.join(data_dir, class_name)\n            for fname in os.listdir(class_dir):\n                fpath = os.path.join(class_dir, fname)\n                if os.path.isfile(fpath) and fname.lower().endswith(('.png','jpg','jpeg')):\n                    self.image_paths.append(fpath)\n                    self.labels.append(idx)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        label = self.labels[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        image = np.array(image) # PIL â†’ HÃ—WÃ—C numpy array\n        if self.transform:\n            augmented = self.transform(image=image) # must pass as keyword\n            image = augmented['image'] # grab the transformed tensor\n\n        return image, label","metadata":{"_uuid":"4cfcae82-0f91-4606-b14d-05bb526183da","_cell_guid":"773ff9e7-0c9b-4199-95bb-93296aafac07","trusted":true,"collapsed":false,"id":"HpcieEGZ30cW","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# transformations\ndef get_train_augmentations(image_size: int):\n    return A.Compose([\n        A.Resize(image_size, image_size),\n        A.HorizontalFlip(p=0.5),\n        # A.RandomBrightness(limit=0.2, p=0.3), # changes image brightness to mimic lighting variation # search for right import\n        # A.RandomContrast(limit=0.2, p=0.3), # modifies contrast to handle visual differences\n        A.Blur(blur_limit=3, p=0.2), # general softening of the image\n        A.MedianBlur(blur_limit=3, p=0.2), # removes noise while keeping edges sharp\n        A.GaussianBlur(blur_limit=(3,5), p=0.2), # natural smooth blur like out-of-focus camera\n        A.MotionBlur(blur_limit=5, p=0.2), # simulates camera shake or patient movement\n        A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=0.3), # lens-like warping of image\n        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3), # distorts image with grid pattern\n        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.3), # adjusts tint, saturation, brightness\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=10, p=0.5), # shifts, zooms, rotates image slightly\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ])\n\ndef get_val_augmentations(image_size: int):\n    return A.Compose([\n        A.Resize(image_size, image_size),\n        A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n        ToTensorV2(),\n    ])","metadata":{"_uuid":"5293ee47-efec-4e1b-aa6c-822732572b73","_cell_guid":"f3f3fb8a-e7f6-455b-9214-a15446da79b2","trusted":true,"collapsed":false,"id":"7GrK9OLT5oz9","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# visualize images\ndef imshow(img_tensor, mean, std):\n    \"\"\"\n    img_tensor: CÃ—HÃ—W torch Tensor, normalized\n    mean, std: sequences of length C\n    returns: HÃ—WÃ—C numpy array in [0,1]\n    \"\"\"\n    # move to CÃ—HÃ—W numpy\n    img = img_tensor.cpu().numpy()\n    # unnormalize per channel\n    for c in range(img.shape[0]):\n        img[c] = img[c] * std[c] + mean[c]\n    # transpose to HÃ—WÃ—C\n    img = np.transpose(img, (1,2,0))\n    # clip to valid range\n    return np.clip(img, 0, 1)\n\ndef show_batch(dataset, class_names, num_samples=16):\n    loader = DataLoader(dataset, batch_size=num_samples, shuffle=True)\n    images, labels = next(iter(loader))\n\n    # constants â€“ must match your Normalize()\n    mean = (0.485, 0.456, 0.406)\n    std  = (0.229, 0.224, 0.225)\n\n    n = int(num_samples**0.5)  # for a 4Ã—4 grid if num_samples=16\n    fig, axes = plt.subplots(n, n, figsize=(n*4, n*4))\n\n    for ax, img_t, lab in zip(axes.flatten(), images, labels):\n        img = imshow(img_t, mean, std)\n        ax.imshow(img)\n        ax.set_title(class_names[lab], fontsize=12)\n        ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n# def show_batch(dataset, class_names, num_samples=16):\n#     loader = DataLoader(dataset, batch_size=num_samples, shuffle=True)\n#     images, labels = next(iter(loader))\n#     # 4Ã—4 grid\n#     n = int(num_samples**0.5)\n#     fig, axes = plt.subplots(n, n, figsize=(n*4, n*4))\n#     for ax, img, lab in zip(axes.flatten(), images, labels):\n#         # img is CÃ—HÃ—W tensor in [0,1]; move to HÃ—WÃ—C:\n#         ax.imshow(img.permute(1, 2, 0).cpu().numpy())\n#         ax.set_title(class_names[lab], fontsize=12)\n#         ax.axis('off')\n#     plt.tight_layout()\n#     plt.show()","metadata":{"_uuid":"ddaee4ca-a29e-4bb7-950b-884c343bd8df","_cell_guid":"5d40d712-7c8d-475c-a252-4f5627d77638","trusted":true,"collapsed":false,"id":"NvU3n4d98oOQ","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# usage\ntrain_dir = \"/kaggle/working/data/train\"\nclass_names = sorted(os.listdir(train_dir))\nraw_dataset = ChestXRayDataset(train_dir, class_names, transform=get_val_augmentations(256))\naug_dataset = ChestXRayDataset(train_dir, class_names, transform=get_train_augmentations(256))\nprint(\"Raw images: \")\nshow_batch(raw_dataset, class_names)","metadata":{"_uuid":"f8729686-6157-4b04-a74e-1264b2406ebd","_cell_guid":"8fb71003-0cae-4ab1-806f-0d2887f4b46a","trusted":true,"collapsed":false,"id":"T57IW_or9Ppj","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Augmented images: \")\nshow_batch(aug_dataset, class_names)","metadata":{"_uuid":"bc86ddaf-ee95-457a-816c-3c031d7d879f","_cell_guid":"43b2af39-fac9-4da2-a144-7f0b36a390ad","trusted":true,"collapsed":false,"id":"1C5XOrDog0Ef","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Model definitions","metadata":{"_uuid":"3ac0d80c-c9ea-4c73-983c-35abdd021b87","_cell_guid":"e071f645-5cbb-42e9-86bc-0d8195010f2d","trusted":true,"collapsed":false,"id":"kSgiLT319qka","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# class PRNetEffNetB5(nn.Module):\n#   def __init__(self, num_classes: int):\n#     super().__init__()\n#     self.backbone = timm.create_model('efficientnet_b5', pretrained=True, features_only=True) # defining own classification head\n#     num_channels = self.backbone.feature_info[-1][\"num_chs\"] # get the most abstract feature_map from the the backbone layer\n#     self.global_pool = nn.AdapativeAvgPool2d(1)\n#     self.classifier_dropout = nn.Dropout(p=0.5)\n#     self.classifier = nn.Linear(num_channels, num_classes)\n\n#   def forward(self, x):\n#     features = self.backbone(x)[-1]\n#     pooled = self.global_pool(features).flatten(1) # takes output of [B, C, H, W] and coverts it to [B, C, 1, 1] or [B, C]\n#     dropped = self.classifier_dropout(pooled)\n#     logits = self.classifier(dropped)\n#     return logits","metadata":{"_uuid":"ea303533-6a50-49ca-a27b-e4e1c18d63f5","_cell_guid":"2f08b7cd-9c7d-437c-825f-6819d2d23884","trusted":true,"collapsed":false,"id":"owKse1Pn9tYO","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EfficientNetClassifier(nn.Module):\n    def __init__(self, name, num_classes):\n        super().__init__()\n        self.backbone = timm.create_model(name, pretrained=True, features_only=True)\n        in_ch = self.backbone.feature_info[-1][\"num_chs\"]\n        self.head = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Dropout(0.5),\n            nn.Linear(in_ch, num_classes)\n        )\n\n    def forward(self, x):\n        feats = self.backbone(x)[-1]  # take the deepest feature map only\n        out = self.head(feats)\n        return out\n\ndef create_effnet_backbone(name, num_classes):\n    return EfficientNetClassifier(name, num_classes).to(device)\n\n# def create_effnet_backbone(name, num_classes):\n#     # name=\"efficientnet_b0\" or \"efficientnet_b5\"\n#     backbone = timm.create_model(name, pretrained=True, features_only=True)\n#     in_ch = backbone.feature_info[-1][\"num_chs\"]  # get the most abstract feature_map from the the backbone layer\n#     head = nn.Sequential(\n#         nn.AdaptiveAvgPool2d(1),\n#         nn.Flatten(),\n#         nn.Dropout(0.5),\n#         nn.Linear(in_ch, num_classes)\n#     )\n#     model = nn.Sequential(backbone, head)\n#     return model.to(device)","metadata":{"_uuid":"26714056-babe-4357-a014-50910499dd0b","_cell_guid":"9ddfbece-ea35-443e-9626-5e18fe240b99","trusted":true,"collapsed":false,"id":"Bxvpc2JUK1xp","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class VisionTransformerStub(nn.Module):\n#     def __init__(self, num_classes: int):\n#         super().__init__()\n#         self.vit = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=num_classes)\n#     def forward(self, x):\n#         return self.vit(x)","metadata":{"_uuid":"1c3d9f28-089c-489d-bc08-298e67b149e8","_cell_guid":"783ad7a4-9e44-4d47-8eeb-6154b4f0ea5b","trusted":true,"collapsed":false,"id":"Iohyf-09_3F4","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Bias-adjustable Softmax","metadata":{"_uuid":"d9357ae7-26ab-45d6-9ef1-6bd123a3213d","_cell_guid":"0592ff88-b6fa-420e-a254-2cfc04ff0a0f","trusted":true,"collapsed":false,"id":"cc8LAqXY_5Yt","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# def bias_softmax(logits, exps):\n#     probs = torch.softmax(logits, dim=-1)\n#     adj   = probs ** torch.tensor(exps, device=probs.device)\n#     return adj / adj.sum(-1, keepdim=True)\n\n# def search_biased_exponents(val_loader, model, exp_range=(0.0,5.0), step=0.1):\n#     model.eval()\n#     all_logits = []\n#     all_labels = []\n#     with torch.no_grad():\n#         for imgs, lbls in val_loader:\n#             imgs = imgs.to(device)\n#             all_logits.append(model(imgs).cpu())\n#             all_labels.append(torch.tensor(lbls))\n#     all_logits = torch.cat(all_logits)\n#     all_labels = torch.cat(all_labels)\n#     best = [1.0]*all_logits.size(1)\n#     for c in range(all_logits.size(1)):\n#         best_acc,best_p=0,1.0\n#         for p in np.arange(exp_range[0], exp_range[1]+1e-9, step):\n#             exps = best.copy()\n#             exps[c] = p\n#             preds = bias_softmax(all_logits, exps).argmax(1)\n#             acc = (preds==all_labels).float().mean().item()\n#             if acc>best_acc: best_acc,best_p=acc,p\n#         best[c]=best_p\n#     return best","metadata":{"_uuid":"51f3003b-17f9-4e52-8aca-8ed69e131d2c","_cell_guid":"0bbf895c-da6b-443d-87f0-7e4dd52a2d9b","trusted":true,"collapsed":false,"id":"orbPTdKBLH87","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bias_softmax(logits, exponents):\n    \"\"\"\n    applies bias-adjustable softmax to modify class probabilities\n\n    how it works:\n    1. get normal softmax probabilities (0 to 1, sum to 1)\n    2. raise each class probability to its own exponent\n       - exponent > 1: makes high probs higher, low probs lower (sharpens)\n       - exponent < 1: makes distribution more uniform (smooths)\n       - exponent = 1: no change (normal softmax)\n    3. renormalize so they still sum to 1\n\n    example:\n    - normal probs: [0.6, 0.3, 0.1]\n    - exponents: [1.0, 0.5, 2.0]\n    - after power: [0.6^1.0, 0.3^0.5, 0.1^2.0] = [0.6, 0.55, 0.01]\n    - after normalize: [0.52, 0.47, 0.01] (approximately)\n\n    args:\n        logits: model outputs before softmax, shape (batch_size, num_classes)\n        exponents: list of exponents for each class, length = num_classes\n\n    returns:\n        adjusted probabilities, same shape as input\n    \"\"\"\n    # step 1: get normal softmax probabilities\n    normal_probs = torch.softmax(logits, dim=-1)\n\n    # step 2: raise each class to its exponent\n    # convert exponents to tensor on same device as probabilities\n    exp_tensor = torch.tensor(exponents, device=normal_probs.device)\n    adjusted_probs = normal_probs ** exp_tensor\n\n    # step 3: renormalize so each row sums to 1\n    # sum across classes (dim=-1) and keep dimension for broadcasting\n    normalized = adjusted_probs / adjusted_probs.sum(dim=-1, keepdim=True)\n\n    return normalized\n\n\ndef search_best_exponents(validation_loader, trained_model, search_range=(0.1, 3.0), step_size=0.1):\n    \"\"\"\n    finds the best exponent for each class using grid search\n\n    the idea:\n    - for each class, try different exponent values\n    - pick the exponent that gives highest validation accuracy\n    - do this one class at a time (greedy search)\n\n    why this works:\n    - if a class is being under-predicted, use exponent > 1 to boost it\n    - if a class is being over-predicted, use exponent < 1 to dampen it\n    - the paper found exponents [1.0, 0.4, 1.6] for covid/pneumonia/normal\n      meaning pneumonia was over-predicted so they dampened it with 0.4\n\n    args:\n        validation_loader: dataloader for validation set\n        trained_model: the model to tune (should be already trained)\n        search_range: (min_exp, max_exp) to search\n        step_size: how fine-grained the search is\n\n    returns:\n        list of best exponents for each class\n    \"\"\"\n    print(\"collecting validation predictions for bias tuning...\")\n\n    # put model in evaluation mode\n    trained_model.eval()\n\n    # collect all validation data predictions in one go\n    # this is more efficient than running inference multiple times\n    all_logits = []  # model outputs (before softmax)\n    all_true_labels = []\n\n    with torch.no_grad():\n        for images, labels in validation_loader:\n            images = images.to(device)\n            logits = trained_model(images)\n            all_logits.append(logits.cpu()) # move back to CPU for storage (saves GPU memory)\n            # labels might already be tensors, but ensure they're tensors\n            if not isinstance(labels, torch.Tensor):\n                labels = torch.tensor(labels)\n            all_true_labels.append(labels)\n\n    # combine all batches into single tensors\n    all_logits = torch.cat(all_logits, dim=0)  # shape: (total_samples, num_classes)\n    all_true_labels = torch.cat(all_true_labels, dim=0)  # shape: (total_samples,)\n    num_classes = all_logits.size(1)\n    print(f\"collected {all_logits.size(0)} samples with {num_classes} classes\")\n\n    # start with default exponents (no bias adjustment)\n    best_exponents = [1.0] * num_classes\n\n    # optimize each class exponent one by one\n    for class_idx in range(num_classes):\n        print(f\"tuning exponent for class {class_idx}...\")\n\n        best_accuracy = 0.0\n        best_exponent = 1.0\n\n        # try different exponent values for this class\n        search_values = np.arange(search_range[0], search_range[1] + step_size/2, step_size)\n\n        for exp_value in search_values:\n            # create exponent list with only this class modified\n            test_exponents = best_exponents.copy()\n            test_exponents[class_idx] = exp_value\n\n            # apply bias-adjustable softmax with these exponents\n            adjusted_probs = bias_softmax(all_logits, test_exponents)\n\n            # get predictions (highest probability class)\n            predictions = adjusted_probs.argmax(dim=1)\n\n            # calculate accuracy\n            correct = (predictions == all_true_labels).float()\n            accuracy = correct.mean().item()\n\n            # keep track of best exponent for this class\n            if accuracy > best_accuracy:\n                best_accuracy = accuracy\n                best_exponent = exp_value\n\n        # update the best exponent for this class\n        best_exponents[class_idx] = best_exponent\n        print(f\"  class {class_idx}: best_exponent={best_exponent:.2f}, accuracy={best_accuracy:.4f}\")\n\n    print(f\"final best exponents: {best_exponents}\")\n    return best_exponents\n\n\n# example usage:\n\"\"\"\n# after training your model, tune the bias exponents\nmodel = your_trained_model\nval_loader = your_validation_dataloader\n\n# find best exponents\noptimal_exponents = search_best_exponents(val_loader, model)\n\n# then during inference on test set:\nwith torch.no_grad():\n    for test_images, test_labels in test_loader:\n        test_images = test_images.to(DEVICE)\n        logits = model(test_images)\n\n        # apply bias adjustment\n        adjusted_probs = bias_softmax(logits, optimal_exponents)\n        predictions = adjusted_probs.argmax(dim=1)\n\n        # calculate test accuracy with bias adjustment\n        ...\n\"\"\"","metadata":{"_uuid":"defbec52-604a-4aaa-9184-0ecaaca8320f","_cell_guid":"ff1b3935-5cc9-4e99-aa8f-97456881d85d","trusted":true,"collapsed":false,"id":"sR7rIQqUcv2B","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Training + Validation","metadata":{"_uuid":"b136f5df-b993-456c-b94f-3ef4ce938ee3","_cell_guid":"7b58c69b-54ca-4a1f-bf82-c3ff9010c9cc","trusted":true,"collapsed":false,"id":"Td8UhQ-nLK6s","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"Dropout (in your model):\n\nRandomly sets some neurons to zero during training\nPrevents neurons from becoming too dependent on each other\nApplied to activations/features\n\nWeight Decay (in optimizer):\n\nPenalizes large weight values directly\nKeeps weights small to prevent overfitting\nApplied to the actual model parameters\n\nWhy Use Both?\nThey target different aspects of overfitting:\nDropout prevents co-adaptation - stops neurons from memorizing specific patterns together\nWeight Decay prevents parameter explosion - stops individual weights from becoming too large","metadata":{"_uuid":"f959d6b4-b128-4e9e-8a4f-49adcdb8660b","_cell_guid":"99958a5e-41a5-4c16-9614-db21965cb94b","trusted":true,"collapsed":false,"id":"U642GZ8xhH57","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def create_warmup_cosine_scheduler(optimizer, total_epochs, warmup_epochs=3, min_lr=1e-7):\n    \"\"\"\n    Returns a SequentialLR that:\n     - linearly warms lr from 0â†’initial over `warmup_epochs`\n     - then cosine-anneals from initialâ†’`min_lr` over remaining epochs\n     - LR = min_lr + 0.5 * (max_lr - min_lr) * (1 + cos(Ï€ * current_step / T_max))\n    \"\"\"\n    # warmup: 0 â†’ 1Ã—LR\n    warmup = LinearLR(\n        optimizer,\n        start_factor=1e-3,\n        end_factor=1.0,\n        total_iters=warmup_epochs\n    )\n    # cosine: 1Ã—LR â†’ min_lr\n    cosine = CosineAnnealingLR(\n        optimizer,\n        T_max=max(total_epochs - warmup_epochs, 1), # ensures atleast T_max = 1 to avoid division by 0 in cosin formular\n        eta_min=min_lr\n    )\n    # join them at warmup_epochs\n    scheduler = SequentialLR(\n        optimizer,\n        schedulers=[warmup, cosine],\n        milestones=[warmup_epochs]\n    )\n    return scheduler","metadata":{"_uuid":"f4ce0631-3174-4b2b-948a-aaa7d960c658","_cell_guid":"4e63173b-d356-4861-8b06-9f0e54463d85","trusted":true,"collapsed":false,"id":"mS7-4wcuUFjW","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_metrics_per_model(results):\n    \"\"\"\n    For each result dict in `results`, creates a figure with two side-by-side plots:\n      - Left: Loss curves (train, val, and horizontal test line)\n      - Right: Accuracy curves (train, val, and horizontal test line)\n\n    Assumes each dict has keys:\n      'ConfigLabel', 'Train Losses', 'Val Losses', 'Test Loss',\n      'Train Accs', 'Val Accs', 'Test Acc'.\n    \"\"\"\n    for r in results:\n        label = r.get('ConfigLabel', r.get('Backbone', 'Model'))\n        epochs = range(1, len(r['Train Losses']) + 1)\n        fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(12, 4))\n\n        # loss plot\n        ax_loss.plot(epochs, r['Train Losses'], label='Train Loss')\n        ax_loss.plot(epochs, r['Val Losses'], label='Val Loss')\n        ax_loss.hlines(r['Test Loss'], xmin=1, xmax=epochs[-1], linestyles='--', label='Test Loss')\n        ax_loss.set_title(f'{label} Loss')\n        ax_loss.set_xlabel('Epoch')\n        ax_loss.set_ylabel('Loss')\n        ax_loss.legend()\n        ax_loss.grid(True)\n\n        # accuracy plot\n        ax_acc.plot(epochs, r['Train Accs'], label='Train Acc')\n        ax_acc.plot(epochs, r['Val Accs'], label='Val Acc')\n        ax_acc.hlines(r['Test Accuracy'], xmin=1, xmax=20, linestyles='--', label='Test Acc')\n        ax_acc.set_title(f'{label} Accuracy')\n        ax_acc.set_xlabel('Epoch')\n        ax_acc.set_ylabel('Accuracy')\n        ax_acc.legend()\n        ax_acc.grid(True)\n\n        plt.tight_layout()\n        filename = f\"{label.replace(' ', '_')}_metrics.png\"\n        plt.savefig(filename)\n        print(f\"Saved plot as {filename}\")\n        display(FileLink(filename))\n        plt.show()","metadata":{"_uuid":"0e9e514f-2abe-48fb-a2e2-0a95d8077db6","_cell_guid":"2feb68ff-e6a3-43f3-b4ce-8a2802d10e8f","trusted":true,"collapsed":false,"id":"UI3FbaJdqvsP","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\nfrom torch.cuda.amp import GradScaler, autocast\n\n# dataset = ChestXRayDataset()  # assumed imported\n\ndef run_configuration(\n    backbone_name: str,\n    image_size: int,\n    use_progressive: bool,\n    use_bas: bool,\n    data_dir: str = \"/kaggle/working/data\",\n    total_epochs: int = 20,\n    batch_size: int = 16,  # base batch size\n    base_lr: float = 1e-4,\n    weight_decay: float = 1e-4,\n    early_stopping_patience: int = 5\n) -> dict:\n    # suppress warnings and logging\n    warnings.filterwarnings(\"ignore\")\n    logging.getLogger().setLevel(logging.ERROR)\n    logging.getLogger(\"timm.models._builder\").setLevel(logging.ERROR)\n\n    # setup\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    class_names = sorted(os.listdir(f\"{data_dir}/train\"))\n    num_classes = len(class_names)\n    model = create_effnet_backbone(backbone_name, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n    scheduler = create_warmup_cosine_scheduler(optimizer, total_epochs)\n\n    # prepare AMP scaler\n    scaler = GradScaler()\n\n    # figure out what image sizes we're gonna use\n    # if progressive: start small and work our way up like the paper says\n    # if not: just use the final size the whole time\n    stages = [image_size] if not use_progressive else [256, 380, 460, 512, image_size]\n    epochs_per_stage = total_epochs // len(stages)\n    prev_ckpt = None\n    best_overall_ckpt = None\n    total_train_time = total_val_time = 0.0\n    best_overall_train_acc = best_overall_val_acc = 0.0\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n\n    for stage_idx, sz in enumerate(stages, 1):\n        # adjust batch size for large resolutions\n        stage_bs = batch_size if sz < 512 else max(1, batch_size // 2)\n        print(f\"starting stage {stage_idx}/{len(stages)}: resolution {sz}x{sz}, batch size {stage_bs}\")\n\n        # load previous weights (if we have one)\n        if prev_ckpt:\n            model.load_state_dict(torch.load(prev_ckpt))\n            print(f\"Loaded checkpoint: {prev_ckpt}\")\n\n        # data loaders\n        train_loader = DataLoader(\n            ChestXRayDataset(f\"{data_dir}/train\", class_names, transform=get_train_augmentations(sz)),\n            batch_size=stage_bs, shuffle=True, num_workers=2\n        )\n        val_loader = DataLoader(\n            ChestXRayDataset(f\"{data_dir}/val\", class_names, transform=get_val_augmentations(sz)),\n            batch_size=stage_bs, shuffle=False, num_workers=2\n        )\n\n        # for early stopping\n        epochs_no_improve = 0\n        best_stage_val_acc = 0.0\n\n        for epoch in range(1, epochs_per_stage + 1):\n            # TRAIN\n            model.train()\n            t0 = time.time()\n            running_loss = correct = total = 0\n            train_bar = tqdm(train_loader, desc=f\"stage {stage_idx} @{sz}px - training epoch {epoch}\", leave=False)\n            for imgs, lbls in train_bar:\n                imgs, lbls = imgs.to(device), lbls.to(device)\n                optimizer.zero_grad()\n\n                # forward pass\n                with autocast():\n                    logits = model(imgs)\n                    loss = criterion(logits, lbls)\n\n                # backward pass\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n\n                # track accuracy for this batch\n                running_loss += loss.item() * lbls.size(0)\n                preds = logits.argmax(1)\n                correct += (preds == lbls).sum().item()\n                total += lbls.size(0)\n\n                # update progress bar with current accuracy\n                current_acc = correct / total\n                train_bar.set_postfix({'acc': f'{current_acc:.3f}'})\n\n            epoch_train_time = time.time() - t0\n            total_train_time += epoch_train_time\n            epoch_train_loss = running_loss / total\n            epoch_train_acc = correct / total\n            train_accs.append(epoch_train_acc)\n            train_losses.append(epoch_train_loss)\n\n            # VALIDATION\n            model.eval()\n            t1 = time.time()\n            val_loss_sum = correct_val = tot_val = 0\n            val_bar = tqdm(val_loader, desc=f\"stage {stage_idx} @{sz}px - validating epoch {epoch}\", leave=False)\n            with torch.no_grad():\n                for imgs, lbls in val_bar:\n                    imgs, lbls = imgs.to(device), lbls.to(device)\n                    with autocast():\n                        logits = model(imgs)\n                        loss = criterion(logits, lbls)\n\n                    val_loss_sum += loss.item() * lbls.size(0)\n                    preds = logits.argmax(1)\n                    correct_val += (preds == lbls).sum().item()\n                    tot_val += lbls.size(0)\n\n                    # show current val accuracy in progress bar\n                    current_val_acc = correct_val / tot_val\n                    val_bar.set_postfix({'val_acc': f'{current_val_acc:.3f}'})\n\n            epoch_val_time = time.time() - t1\n            total_val_time += epoch_val_time\n            epoch_val_acc = correct_val / tot_val\n            epoch_val_loss = val_loss_sum / tot_val\n            val_accs.append(epoch_val_acc)\n            val_losses.append(epoch_val_loss)\n\n            print(f\"\\nEpoch {epoch}: train_acc={epoch_train_acc:.4f}, val_acc={epoch_val_acc:.4f}, train_loss={epoch_train_loss:.4f}, val_loss={epoch_val_loss:.4f}\")\n\n            # clear cache\n            torch.cuda.empty_cache()\n\n            ## check if this is the best validation accuracy we've seen in this stage\n            if epoch_val_acc > best_stage_val_acc:\n                best_stage_val_acc = epoch_val_acc\n                epochs_no_improve = 0\n                \n                # save the model - this is our best checkpoint so far\n                prev_ckpt = f\"{backbone_name}_stage{stage_idx}_{sz}.pth\"\n                torch.save(model.state_dict(), prev_ckpt)\n                print(f\"new best val acc: {epoch_val_acc:.4f} - saved to {prev_ckpt}\")\n                \n                # update our global best accuracies if this beats them\n                if epoch_train_acc > best_overall_train_acc:\n                    best_overall_train_acc = epoch_train_acc\n                    print(f\"new overall best train acc: {epoch_train_acc:.4f}\")\n                    \n                if epoch_val_acc > best_overall_val_acc:\n                    best_overall_val_acc = epoch_val_acc\n                    best_overall_ckpt = prev_ckpt\n                    print(f\"new overall best val acc: {epoch_val_acc:.4f}\")\n                    \n            else:\n                epochs_no_improve += 1\n                print(f\"no improvement for {epochs_no_improve} epochs\")\n                \n                # early stopping check\n                if epochs_no_improve >= early_stopping_patience:\n                    print(f\"early stopping triggered at stage {stage_idx}, epoch {epoch}\")\n                    break\n\n            # update learning rate\n            scheduler.step()\n\n        # if we early stopped, don't continue to next stages\n        if epochs_no_improve >= early_stopping_patience:\n            print(\"\\nearly stopping triggered - stopping all stages\")\n            break\n        print(\"\\n\")\n\n    # load up the best model we found\n    if best_overall_ckpt:\n        model.load_state_dict(torch.load(best_overall_ckpt))\n        # print(f\"loaded best model from {prev_ckpt}\")\n        print(f\"âœ… loaded best validation model from {best_overall_ckpt} (val_acc={best_overall_val_acc:.4f})\")\n\n    # bias-adjustable softmax tuning (only if requested)\n    best_exps = [1.0] * num_classes  # default exponents\n    if use_bas:\n        print(\"tuning bias-adjustable softmax on validation set...\")\n        final_val_loader = DataLoader(\n            ChestXRayDataset(f\"{data_dir}/val\", class_names, transform=get_val_augmentations(image_size)),\n            batch_size=batch_size, shuffle=False, num_workers=4\n        )\n        best_exps = search_best_exponents(final_val_loader, model)\n        print(f\"best bias exponents: {best_exps}\")\n\n    # final test evaluation\n    print(\"running final test evaluation...\")\n    t2 = time.time()\n    total_test_loss = correct_test = tot_test = 0\n    \n    test_loader = DataLoader(\n        ChestXRayDataset(f\"{data_dir}/test\", class_names, transform=get_val_augmentations(image_size)),\n        batch_size=batch_size, shuffle=False, num_workers=4\n    )\n    \n    model.eval()\n    with torch.no_grad():\n        for imgs, lbls in tqdm(test_loader, desc=\"Final test evaluation\"):\n            imgs, lbls = imgs.to(device), lbls.to(device)\n            logits = model(imgs)\n            loss = criterion(logits, lbls)\n            total_test_loss += loss.item() * lbls.size(0)\n            \n            # apply bias-adjustable softmax if we're using it\n            if use_bas:\n                logits = bias_softmax(logits, best_exps)\n                \n            preds = logits.argmax(1)\n            correct_test += (preds==lbls).sum().item()\n            tot_test += lbls.size(0)\n            \n    test_acc  = correct_test / tot_test\n    test_loss = total_test_loss / tot_test\n    test_time = time.time() - t2\n    \n    print(f\"\\nfinal results:\")\n    print(f\"  best train acc (across all stages): {best_overall_train_acc:.4f}\")\n    print(f\"  best val acc (across all stages): {best_overall_val_acc:.4f}\")\n    print(f\"  final test acc: {test_acc:.4f}\")\n  \n    warnings.filterwarnings(\"ignore\")\n\n    # return results in the format that matches the paper's table (fix backbone name if running multiple models)\n    return {\n        \"Backbone\":       backbone_name.replace(\"efficientnet_\",\"EffNet-\").upper(), \n        \"Image Size\":     f\"{image_size}Ã—{image_size}\",\n        \"Progressive\":    \"Yes\" if use_progressive else \"No\",\n        \"Bias-Softmax\":   \"Yes\" if use_bas else \"No\",\n        \"5-Fold Accuracy\": f\"{best_overall_train_acc*100:.2f}%\",  # best train acc seen\n        \"Validation Accuracy\": f\"{best_overall_val_acc*100:.2f}%\",  # best val acc seen\n        \"Test Accuracy\":  f\"{test_acc*100:.2f}%\",  # final test performance\n        \"Train Time (s)\": f\"{total_train_time:.1f}\",\n        \"Val Time (s)\":   f\"{total_val_time:.1f}\",\n        \"Test Time (s)\":  f\"{test_time:.1f}\",\n        \"Test Loss\": f\"{test_loss:.4f}\",\n        \"Test Acc\": f\"{test_acc:.4f}\",\n        # for plotting only:\n        \"Train Losses\": train_losses,\n        \"Val Losses\": val_losses,\n        \"Train Accs\": [a * 100 for a in train_accs],\n        \"Val Accs\": [a * 100 for a in val_accs],\n    }","metadata":{"_uuid":"ef6e505d-3ea5-4ff1-8fb2-d3dd5cc85de8","_cell_guid":"3380d276-7f0e-465b-82d3-3a6585fa5e4b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"configs = [\n    # (\"efficientnet_b0\", 512, False, False),  # EffNet-B0, 512x512, no progressive, no bias\n    # (\"efficientnet_b0\", 512, False, True),   # EffNet-B0, 512x512, no progressive, with bias\n    # (\"efficientnet_b0\", 640, True,  False),  # EffNet-B0, 640x640, with progressive, no bias\n    # (\"efficientnet_b0\", 640, True,  True),   # EffNet-B0, 640x640, with progressive, with bias\n    # (\"efficientnet_b5\", 512, False, False),  # EffNet-B5, 512x512, no progressive, no bias\n    # (\"efficientnet_b5\", 512, False, True),   # EffNet-B5, 512x512, no progressive, with bias\n    # (\"efficientnet_b5\", 640, True,  False),  # EffNet-B5, 640x640, with progressive, no bias\n    (\"efficientnet_b5\", 640, True,  True),   # EffNet-B5, 640x640, with progressive, with bias (their best)\n]\n\n# run all experiments\nresults = []\nfor i, (backbone, size, prog, bas) in enumerate(configs, 1):\n    print(f\"\\nðŸš€ experiment {i}/{len(configs)}: {backbone} | size={size} | progressive={prog} | bias_softmax={bas}\")\n\n    res = run_configuration(\n        backbone_name=backbone,\n        image_size=size,\n        use_progressive=prog,\n        use_bas=bas,\n        total_epochs=20,\n        batch_size=16,\n        early_stopping_patience=5\n    )\n\n    results.append(res)\n    print(f\"âœ… completed: test_acc = {res['Test Accuracy']}\")\n\n# create the results table like in the paper\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL RESULTS TABLE (reproducing paper's Table I)\")\nprint(\"=\"*80)","metadata":{"_uuid":"392451ec-c188-47e0-9f4f-0a981f5cbf55","_cell_guid":"b942f99e-2989-42b7-8b86-6b3a19e14b62","trusted":true,"collapsed":false,"id":"NT21lZEgLXJw","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results","metadata":{"_uuid":"12da0187-5099-4b50-a4a1-b190e75622e2","_cell_guid":"1d2d69b7-594f-47fe-880f-193a9014b856","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot loss and accuracy curves\nplot_metrics_per_model(results)","metadata":{"_uuid":"91b6ae9d-7421-475f-8488-40b9b7b52be4","_cell_guid":"dee076b3-6a09-488a-b4ad-e6926d71da63","trusted":true,"collapsed":false,"id":"h9xpm_9Ix1s6","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# save results\nflat_results = []\nfor r in results:\n    flat = {k: v for k, v in r.items() if not isinstance(v, list)}\n    flat_results.append(flat)\n\ndf = pd.DataFrame(flat_results)\ndf.to_csv(\"prnet_results.csv\", index=False)\nprint(f\"\\nresults saved to prnet_results.csv\")\n\n# display results\nfrom IPython.display import display, FileLink\ndf = pd.read_csv(\"prnet_results.csv\")\ndisplay(df)","metadata":{"_uuid":"efeb64eb-4a35-4fab-9dc4-204f73ccbbfb","_cell_guid":"6166d11f-0661-4816-aba1-cada9a89676d","trusted":true,"collapsed":false,"id":"A_ypG16iLYZj","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train.py\n# 0. imports\nimport os, random, time\nfrom glob import glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport time\nfrom tqdm.auto import tqdm\nimport shutil\nimport warnings\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom torchvision import transforms, utils\nfrom torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\nimport timm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport argparse\nimport sys\nfrom IPython.display import display, FileLink\nsys.stdout.reconfigure(line_buffering=True)\n\n# 0.1 fix random seeds for reproducibility\ndef set_global_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_global_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# 1.1 LOAD DATASET\nimport kagglehub\n# Download latest version\npath = kagglehub.dataset_download(\"tawsifurrahman/covid19-radiography-database\")\nprint(\"Path to dataset files:\", path)\n\n# List folders/files inside the dataset root directory\nfor root, dirs, files in os.walk(path):\n    print(f\"\\nInspecting folder: {root}\")\n    print(\"Subdirectories:\", dirs)\n    # print(\"Files:\", files)\n    # break  # Only list top-level directories (remove this to go deeper)\n\n# set the base paths\nbase_dataset_path = os.path.join(path, \"COVID-19_Radiography_Dataset\")\noutput_base = \"data\"\nsplits = ['train', 'val', 'test']\nsplit_ratio = [0.8, 0.1, 0.1]  # 80% train, 10% val, 10% test\nclasses = ['COVID', 'Normal', 'Viral Pneumonia', 'Lung_Opacity']\n\n# create output directories\nfor split in splits:\n    for class_name in classes:\n        os.makedirs(os.path.join(output_base, split, class_name), exist_ok=True)\n\n# function to split and copy images\nfor class_name in classes:\n    source_dir = os.path.join(base_dataset_path, class_name, \"images\")\n    all_images = os.listdir(source_dir)\n    random.shuffle(all_images)\n    n_total = len(all_images)\n    n_train = int(split_ratio[0] * n_total)\n    n_val = int(split_ratio[1] * n_total)\n    train_images = all_images[:n_train]\n    val_images = all_images[n_train:n_train + n_val]\n    test_images = all_images[n_train + n_val:]\n\n    def copy_images(image_list, split_name):\n        for image in tqdm(image_list, desc=f\"{split_name} - {class_name}\"):\n            src = os.path.join(source_dir, image)\n            dst = os.path.join(output_base, split_name, class_name, image)\n            shutil.copyfile(src, dst)\n    copy_images(train_images, \"train\")\n    copy_images(val_images, \"val\")\n    copy_images(test_images, \"test\")\n\n# 1.2 CREATE DATASET\nclass ChestXRayDataset(Dataset):\n    def __init__(self, data_dir, class_names, transform=None):\n        self.image_paths = []\n        self.labels = []\n        for idx, class_name in enumerate(class_names):\n            class_dir = os.path.join(data_dir, class_name)\n            for fname in os.listdir(class_dir):\n                fpath = os.path.join(class_dir, fname)\n                if os.path.isfile(fpath) and fname.lower().endswith(('.png','jpg','jpeg')):\n                    self.image_paths.append(fpath)\n                    self.labels.append(idx)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        label = self.labels[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        image = np.array(image) # PIL â†’ HÃ—WÃ—C numpy array\n        if self.transform:\n            augmented = self.transform(image=image) # must pass as keyword\n            image = augmented['image'] # grab the transformed tensor\n        return image, label\n\n# 1.3 TRANSFORMATIONS\ndef get_train_augmentations(image_size: int):\n    return A.Compose([\n        A.Resize(image_size, image_size),\n        A.HorizontalFlip(p=0.5),\n        # A.RandomBrightness(limit=0.2, p=0.3), # changes image brightness to mimic lighting variation # search for right import\n        # A.RandomContrast(limit=0.2, p=0.3), # modifies contrast to handle visual differences\n        A.Blur(blur_limit=3, p=0.2), # general softening of the image\n        A.MedianBlur(blur_limit=3, p=0.2), # removes noise while keeping edges sharp\n        A.GaussianBlur(blur_limit=(3,5), p=0.2), # natural smooth blur like out-of-focus camera\n        A.MotionBlur(blur_limit=5, p=0.2), # simulates camera shake or patient movement\n        A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=0.3), # lens-like warping of image\n        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3), # distorts image with grid pattern\n        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.3), # adjusts tint, saturation, brightness\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=10, p=0.5), # shifts, zooms, rotates image slightly\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ])\n\ndef get_val_augmentations(image_size: int):\n    return A.Compose([\n        A.Resize(image_size, image_size),\n        A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n        ToTensorV2(),\n    ])\n\n# 1.4 VISUALIZE IMAGES\ndef imshow(img_tensor, mean, std):\n    \"\"\"\n    img_tensor: CÃ—HÃ—W torch Tensor, normalized\n    mean, std: sequences of length C\n    returns: HÃ—WÃ—C numpy array in [0,1]\n    \"\"\"\n    # move to CÃ—HÃ—W numpy\n    img = img_tensor.cpu().numpy()\n    # unnormalize per channel\n    for c in range(img.shape[0]):\n        img[c] = img[c] * std[c] + mean[c]\n    # transpose to HÃ—WÃ—C\n    img = np.transpose(img, (1,2,0))\n    # clip to valid range\n    return np.clip(img, 0, 1)\n\ndef show_batch(dataset, class_names, num_samples=16):\n    loader = DataLoader(dataset, batch_size=num_samples, shuffle=True)\n    images, labels = next(iter(loader))\n\n    # constants â€“ must match your Normalize()\n    mean = (0.485, 0.456, 0.406)\n    std  = (0.229, 0.224, 0.225)\n\n    n = int(num_samples**0.5)  # for a 4Ã—4 grid if num_samples=16\n    fig, axes = plt.subplots(n, n, figsize=(n*4, n*4))\n\n    for ax, img_t, lab in zip(axes.flatten(), images, labels):\n        img = imshow(img_t, mean, std)\n        ax.imshow(img)\n        ax.set_title(class_names[lab], fontsize=12)\n        ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n# 1.5 usage\ntrain_dir = \"/kaggle/working/data/train\"\nclass_names = sorted(os.listdir(train_dir))\nraw_dataset = ChestXRayDataset(train_dir, class_names, transform=get_val_augmentations(256))\naug_dataset = ChestXRayDataset(train_dir, class_names, transform=get_train_augmentations(256))\n# print(\"Raw images: \")\n# show_batch(raw_dataset, class_names)\n# print(\"Augmented images: \")\n# show_batch(aug_dataset, class_names)\n\n# 2. MODEL\nclass EfficientNetClassifier(nn.Module):\n    def __init__(self, name, num_classes):\n        super().__init__()\n        self.backbone = timm.create_model(name, pretrained=True, features_only=True)\n        in_ch = self.backbone.feature_info[-1][\"num_chs\"]\n        self.head = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Dropout(0.5),\n            nn.Linear(in_ch, num_classes)\n        )\n\n    def forward(self, x):\n        feats = self.backbone(x)[-1]  # take the deepest feature map only\n        out = self.head(feats)\n        return out\n\ndef create_effnet_backbone(name, num_classes):\n    return EfficientNetClassifier(name, num_classes).to(device)\n\n# 3. BAS\ndef bias_softmax(logits, exponents):\n    \"\"\"\n    applies bias-adjustable softmax to modify class probabilities\n\n    how it works:\n    1. get normal softmax probabilities (0 to 1, sum to 1)\n    2. raise each class probability to its own exponent\n       - exponent > 1: makes high probs higher, low probs lower (sharpens)\n       - exponent < 1: makes distribution more uniform (smooths)\n       - exponent = 1: no change (normal softmax)\n    3. renormalize so they still sum to 1\n\n    example:\n    - normal probs: [0.6, 0.3, 0.1]\n    - exponents: [1.0, 0.5, 2.0]\n    - after power: [0.6^1.0, 0.3^0.5, 0.1^2.0] = [0.6, 0.55, 0.01]\n    - after normalize: [0.52, 0.47, 0.01] (approximately)\n\n    args:\n        logits: model outputs before softmax, shape (batch_size, num_classes)\n        exponents: list of exponents for each class, length = num_classes\n\n    returns:\n        adjusted probabilities, same shape as input\n    \"\"\"\n    # step 1: get normal softmax probabilities\n    normal_probs = torch.softmax(logits, dim=-1)\n\n    # step 2: raise each class to its exponent\n    # convert exponents to tensor on same device as probabilities\n    exp_tensor = torch.tensor(exponents, device=normal_probs.device)\n    adjusted_probs = normal_probs ** exp_tensor\n\n    # step 3: renormalize so each row sums to 1\n    # sum across classes (dim=-1) and keep dimension for broadcasting\n    normalized = adjusted_probs / adjusted_probs.sum(dim=-1, keepdim=True)\n\n    return normalized\n\n\ndef search_best_exponents(validation_loader, trained_model, search_range=(0.1, 3.0), step_size=0.1):\n    \"\"\"\n    finds the best exponent for each class using grid search\n\n    the idea:\n    - for each class, try different exponent values\n    - pick the exponent that gives highest validation accuracy\n    - do this one class at a time (greedy search)\n\n    why this works:\n    - if a class is being under-predicted, use exponent > 1 to boost it\n    - if a class is being over-predicted, use exponent < 1 to dampen it\n    - the paper found exponents [1.0, 0.4, 1.6] for covid/pneumonia/normal\n      meaning pneumonia was over-predicted so they dampened it with 0.4\n\n    args:\n        validation_loader: dataloader for validation set\n        trained_model: the model to tune (should be already trained)\n        search_range: (min_exp, max_exp) to search\n        step_size: how fine-grained the search is\n\n    returns:\n        list of best exponents for each class\n    \"\"\"\n    print(\"collecting validation predictions for bias tuning...\")\n\n    # put model in evaluation mode\n    trained_model.eval()\n\n    # collect all validation data predictions in one go\n    # this is more efficient than running inference multiple times\n    all_logits = []  # model outputs (before softmax)\n    all_true_labels = []\n\n    with torch.no_grad():\n        for images, labels in validation_loader:\n            images = images.to(device)\n            logits = trained_model(images)\n            all_logits.append(logits.cpu()) # move back to CPU for storage (saves GPU memory)\n            # labels might already be tensors, but ensure they're tensors\n            if not isinstance(labels, torch.Tensor):\n                labels = torch.tensor(labels)\n            all_true_labels.append(labels)\n\n    # combine all batches into single tensors\n    all_logits = torch.cat(all_logits, dim=0)  # shape: (total_samples, num_classes)\n    all_true_labels = torch.cat(all_true_labels, dim=0)  # shape: (total_samples,)\n    num_classes = all_logits.size(1)\n    print(f\"collected {all_logits.size(0)} samples with {num_classes} classes\")\n\n    # start with default exponents (no bias adjustment)\n    best_exponents = [1.0] * num_classes\n\n    # optimize each class exponent one by one\n    for class_idx in range(num_classes):\n        print(f\"tuning exponent for class {class_idx}...\")\n\n        best_accuracy = 0.0\n        best_exponent = 1.0\n\n        # try different exponent values for this class\n        search_values = np.arange(search_range[0], search_range[1] + step_size/2, step_size)\n\n        for exp_value in search_values:\n            # create exponent list with only this class modified\n            test_exponents = best_exponents.copy()\n            test_exponents[class_idx] = exp_value\n\n            # apply bias-adjustable softmax with these exponents\n            adjusted_probs = bias_softmax(all_logits, test_exponents)\n\n            # get predictions (highest probability class)\n            predictions = adjusted_probs.argmax(dim=1)\n\n            # calculate accuracy\n            correct = (predictions == all_true_labels).float()\n            accuracy = correct.mean().item()\n\n            # keep track of best exponent for this class\n            if accuracy > best_accuracy:\n                best_accuracy = accuracy\n                best_exponent = exp_value\n\n        # update the best exponent for this class\n        best_exponents[class_idx] = best_exponent\n        print(f\"  class {class_idx}: best_exponent={best_exponent:.2f}, accuracy={best_accuracy:.4f}\")\n\n    print(f\"final best exponents: {best_exponents}\")\n    return best_exponents\n\n# 4.1 TRAINING HELPERS\ndef create_warmup_cosine_scheduler(optimizer, total_epochs, warmup_epochs=3, min_lr=1e-7):\n    \"\"\"\n    Returns a SequentialLR that:\n     - linearly warms lr from 0â†’initial over `warmup_epochs`\n     - then cosine-anneals from initialâ†’`min_lr` over remaining epochs\n     - LR = min_lr + 0.5 * (max_lr - min_lr) * (1 + cos(Ï€ * current_step / T_max))\n    \"\"\"\n    # warmup: 0 â†’ 1Ã—LR\n    warmup = LinearLR(\n        optimizer,\n        start_factor=1e-3,\n        end_factor=1.0,\n        total_iters=warmup_epochs\n    )\n    # cosine: 1Ã—LR â†’ min_lr\n    cosine = CosineAnnealingLR(\n        optimizer,\n        T_max=max(total_epochs - warmup_epochs, 1), # ensures atleast T_max = 1 to avoid division by 0 in cosin formular\n        eta_min=min_lr\n    )\n    # join them at warmup_epochs\n    scheduler = SequentialLR(\n        optimizer,\n        schedulers=[warmup, cosine],\n        milestones=[warmup_epochs]\n    )\n    return scheduler\n\ndef plot_metrics_per_model(results, run_name, outdir='outputs/curves'):\n    \"\"\"\n    For each result dict in `results`, creates a figure with two side-by-side plots:\n      - Left: Loss curves (train, val, and horizontal test line)\n      - Right: Accuracy curves (train, val, and horizontal test line)\n\n    Assumes each dict has keys:\n      'ConfigLabel', 'Train Losses', 'Val Losses', 'Test Loss',\n      'Train Accs', 'Val Accs', 'Test Acc'.\n    \"\"\"\n    if isinstance(results, dict):\n        results = [results]\n    for r in results:\n        label = r.get('ConfigLabel', r.get('Backbone', 'Model'))\n        # run_name = label.replace(' ', '_')\n        epochs = range(1, len(r['Train Losses']) + 1)\n        fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(12, 4))\n\n        # loss plot\n        ax_loss.plot(epochs, r['Train Losses'], label='Train Loss')\n        ax_loss.plot(epochs, r['Val Losses'], label='Val Loss')\n        ax_loss.hlines(r['Test Loss'], xmin=1, xmax=epochs[-1], linestyles='--', label='Test Loss')\n        ax_loss.set_title(f'{label} Loss')\n        ax_loss.set_xlabel('Epoch')\n        ax_loss.set_ylabel('Loss')\n        ax_loss.legend()\n        ax_loss.grid(True)\n\n        # accuracy plot\n        ax_acc.plot(epochs, r['Train Accs'], label='Train Acc')\n        ax_acc.plot(epochs, r['Val Accs'], label='Val Acc')\n        ax_acc.hlines(r['Test Accuracy'], xmin=1, xmax=20, linestyles='--', label='Test Acc')\n        ax_acc.set_title(f'{label} Accuracy')\n        ax_acc.set_xlabel('Epoch')\n        ax_acc.set_ylabel('Accuracy')\n        ax_acc.legend()\n        ax_acc.grid(True)\n\n        plt.tight_layout()\n        filepath = os.path.join(outdir, f\"{run_name}.png\")\n        plt.savefig(filepath)\n        print(f\"Saved plot as {filepath}\")\n        display(FileLink(filepath))\n        plt.show()\n\n# 4.2 RUN CONFIG\n\nimport logging\nfrom torch.cuda.amp import GradScaler, autocast\n\n# dataset = ChestXRayDataset()  # assumed imported\n\ndef run_configuration(\n    backbone_name: str,\n    image_size: int,\n    use_progressive: bool,\n    use_bas: bool,\n    data_dir: str = \"/kaggle/working/data\",\n    total_epochs: int = 20,\n    batch_size: int = 16,  # base batch size\n    base_lr: float = 1e-4,\n    weight_decay: float = 1e-4,\n    early_stopping_patience: int = 5\n) -> dict:\n    # suppress warnings and logging\n    warnings.filterwarnings(\"ignore\")\n    logging.getLogger().setLevel(logging.ERROR)\n    logging.getLogger(\"timm.models._builder\").setLevel(logging.ERROR)\n\n    # setup\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    class_names = sorted(os.listdir(f\"{data_dir}/train\"))\n    num_classes = len(class_names)\n    model = create_effnet_backbone(backbone_name, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n    scheduler = create_warmup_cosine_scheduler(optimizer, total_epochs)\n\n    # prepare AMP scaler\n    scaler = GradScaler()\n\n    # figure out what image sizes we're gonna use\n    # if progressive: start small and work our way up like the paper says\n    # if not: just use the final size the whole time\n    stages = [image_size] if not use_progressive else [256, 380, 460, 512, image_size]\n    epochs_per_stage = total_epochs // len(stages)\n    prev_ckpt = None\n    best_overall_ckpt = None\n    total_train_time = total_val_time = 0.0\n    best_overall_train_acc = best_overall_val_acc = 0.0\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n\n    for stage_idx, sz in enumerate(stages, 1):\n        # adjust batch size for large resolutions\n        stage_bs = batch_size if sz < 512 else max(1, batch_size // 2)\n        print(f\"starting stage {stage_idx}/{len(stages)}: resolution {sz}x{sz}, batch size {stage_bs}\")\n\n        # load previous weights (if we have one)\n        if prev_ckpt:\n            model.load_state_dict(torch.load(prev_ckpt))\n            print(f\"Loaded checkpoint: {prev_ckpt}\")\n\n        # data loaders\n        train_loader = DataLoader(\n            ChestXRayDataset(f\"{data_dir}/train\", class_names, transform=get_train_augmentations(sz)),\n            batch_size=stage_bs, shuffle=True, num_workers=2\n        )\n        val_loader = DataLoader(\n            ChestXRayDataset(f\"{data_dir}/val\", class_names, transform=get_val_augmentations(sz)),\n            batch_size=stage_bs, shuffle=False, num_workers=2\n        )\n\n        # for early stopping\n        epochs_no_improve = 0\n        best_stage_val_acc = 0.0\n\n        for epoch in range(1, epochs_per_stage + 1):\n            # TRAIN\n            model.train()\n            t0 = time.time()\n            running_loss = correct = total = 0\n            train_bar = tqdm(train_loader, desc=f\"stage {stage_idx} @{sz}px - training epoch {epoch}\", leave=False, file=sys.stdout)\n            for imgs, lbls in train_bar:\n                imgs, lbls = imgs.to(device), lbls.to(device)\n                optimizer.zero_grad()\n\n                # forward pass\n                with autocast():\n                    logits = model(imgs)\n                    loss = criterion(logits, lbls)\n\n                # backward pass\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n\n                # track accuracy for this batch\n                running_loss += loss.item() * lbls.size(0)\n                preds = logits.argmax(1)\n                correct += (preds == lbls).sum().item()\n                total += lbls.size(0)\n\n                # update progress bar with current accuracy\n                current_acc = correct / total\n                train_bar.set_postfix({'acc': f'{current_acc:.3f}'})\n\n            epoch_train_time = time.time() - t0\n            total_train_time += epoch_train_time\n            epoch_train_loss = running_loss / total\n            epoch_train_acc = correct / total\n            train_accs.append(epoch_train_acc)\n            train_losses.append(epoch_train_loss)\n\n            # VALIDATION\n            model.eval()\n            t1 = time.time()\n            val_loss_sum = correct_val = tot_val = 0\n            val_bar = tqdm(val_loader, desc=f\"stage {stage_idx} @{sz}px - validating epoch {epoch}\", leave=False, file=sys.stdout)\n            with torch.no_grad():\n                for imgs, lbls in val_bar:\n                    imgs, lbls = imgs.to(device), lbls.to(device)\n                    with autocast():\n                        logits = model(imgs)\n                        loss = criterion(logits, lbls)\n\n                    val_loss_sum += loss.item() * lbls.size(0)\n                    preds = logits.argmax(1)\n                    correct_val += (preds == lbls).sum().item()\n                    tot_val += lbls.size(0)\n\n                    # show current val accuracy in progress bar\n                    current_val_acc = correct_val / tot_val\n                    val_bar.set_postfix({'val_acc': f'{current_val_acc:.3f}'})\n\n            epoch_val_time = time.time() - t1\n            total_val_time += epoch_val_time\n            epoch_val_acc = correct_val / tot_val\n            epoch_val_loss = val_loss_sum / tot_val\n            val_accs.append(epoch_val_acc)\n            val_losses.append(epoch_val_loss)\n\n            print(f\"\\nEpoch {epoch}: train_acc={epoch_train_acc:.4f}, val_acc={epoch_val_acc:.4f}, train_loss={epoch_train_loss:.4f}, val_loss={epoch_val_loss:.4f}\")\n\n            # clear cache\n            torch.cuda.empty_cache()\n\n            ## check if this is the best validation accuracy we've seen in this stage\n            if epoch_val_acc > best_stage_val_acc:\n                best_stage_val_acc = epoch_val_acc\n                epochs_no_improve = 0\n                \n                # save the model - this is our best checkpoint so far\n                prev_ckpt = f\"{backbone_name}_stage{stage_idx}_{sz}.pth\"\n                torch.save(model.state_dict(), prev_ckpt)\n                print(f\"new best val acc: {epoch_val_acc:.4f} - saved to {prev_ckpt}\")\n                \n                # update our global best accuracies if this beats them\n                if epoch_train_acc > best_overall_train_acc:\n                    best_overall_train_acc = epoch_train_acc\n                    print(f\"new overall best train acc: {epoch_train_acc:.4f}\")\n                    \n                if epoch_val_acc > best_overall_val_acc:\n                    best_overall_val_acc = epoch_val_acc\n                    best_overall_ckpt = prev_ckpt\n                    print(f\"new overall best val acc: {epoch_val_acc:.4f}\")\n                    \n            else:\n                epochs_no_improve += 1\n                print(f\"no improvement for {epochs_no_improve} epochs\")\n                \n                # early stopping check\n                if epochs_no_improve >= early_stopping_patience:\n                    print(f\"early stopping triggered at stage {stage_idx}, epoch {epoch}\")\n                    break\n\n            # update learning rate\n            scheduler.step()\n\n        # if we early stopped, don't continue to next stages\n        if epochs_no_improve >= early_stopping_patience:\n            print(\"\\nearly stopping triggered - stopping all stages\")\n            break\n        print(\"\\n\")\n\n    # load up the best model we found\n    if best_overall_ckpt:\n        model.load_state_dict(torch.load(best_overall_ckpt))\n        # print(f\"loaded best model from {prev_ckpt}\")\n        print(f\"âœ… loaded best validation model from {best_overall_ckpt} (val_acc={best_overall_val_acc:.4f})\")\n\n    # bias-adjustable softmax tuning (only if requested)\n    best_exps = [1.0] * num_classes  # default exponents\n    if use_bas:\n        print(\"tuning bias-adjustable softmax on validation set...\")\n        final_val_loader = DataLoader(\n            ChestXRayDataset(f\"{data_dir}/val\", class_names, transform=get_val_augmentations(image_size)),\n            batch_size=batch_size, shuffle=False, num_workers=4\n        )\n        best_exps = search_best_exponents(final_val_loader, model)\n        print(f\"best bias exponents: {best_exps}\")\n\n    # final test evaluation\n    print(\"running final test evaluation...\")\n    t2 = time.time()\n    total_test_loss = correct_test = tot_test = 0\n    \n    test_loader = DataLoader(\n        ChestXRayDataset(f\"{data_dir}/test\", class_names, transform=get_val_augmentations(image_size)),\n        batch_size=batch_size, shuffle=False, num_workers=4\n    )\n    \n    model.eval()\n    with torch.no_grad():\n        for imgs, lbls in tqdm(test_loader, desc=\"Final test evaluation\", file=sys.stdout):\n            imgs, lbls = imgs.to(device), lbls.to(device)\n            logits = model(imgs)\n            loss = criterion(logits, lbls)\n            total_test_loss += loss.item() * lbls.size(0)\n            \n            # apply bias-adjustable softmax if we're using it\n            if use_bas:\n                logits = bias_softmax(logits, best_exps)\n                \n            preds = logits.argmax(1)\n            correct_test += (preds==lbls).sum().item()\n            tot_test += lbls.size(0)\n            \n    test_acc  = correct_test / tot_test\n    test_loss = total_test_loss / tot_test\n    test_time = time.time() - t2\n    \n    print(f\"\\nfinal results:\")\n    print(f\"  best train acc (across all stages): {best_overall_train_acc:.4f}\")\n    print(f\"  best val acc (across all stages): {best_overall_val_acc:.4f}\")\n    print(f\"  final test acc: {test_acc:.4f}\")\n  \n    warnings.filterwarnings(\"ignore\")\n\n    # return results in the format that matches the paper's table (fix backbone name if running multiple models)\n    return {\n        \"Backbone\":       backbone_name.replace(\"efficientnet_\",\"EffNet-\").upper(), \n        \"Image Size\":     f\"{image_size}Ã—{image_size}\",\n        \"Progressive\":    \"Yes\" if use_progressive else \"No\",\n        \"Bias-Softmax\":   \"Yes\" if use_bas else \"No\",\n        \"5-Fold Accuracy\": f\"{best_overall_train_acc*100:.2f}%\",  # best train acc seen\n        \"Validation Accuracy\": f\"{best_overall_val_acc*100:.2f}%\",  # best val acc seen\n        \"Test Accuracy\":  f\"{test_acc*100:.2f}%\",  # final test performance\n        \"Train Time (s)\": f\"{total_train_time:.1f}\",\n        \"Val Time (s)\":   f\"{total_val_time:.1f}\",\n        \"Test Time (s)\":  f\"{test_time:.1f}\",\n        \"Test Loss\": f\"{test_loss:.4f}\",\n        \"Test Acc\": f\"{test_acc:.4f}\",\n        # for plotting only:\n        \"Train Losses\": train_losses,\n        \"Val Losses\": val_losses,\n        \"Train Accs\": [a * 100 for a in train_accs],\n        \"Val Accs\": [a * 100 for a in val_accs],\n    }\n\n# === CLI Entrypoint ===\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Train ChestXRay model\")\n    parser.add_argument(\"--backbone\",    type=str,   required=True)\n    parser.add_argument(\"--image-size\",  type=int,   required=True)\n    parser.add_argument(\"--progressive\", action=\"store_true\")\n    parser.add_argument(\"--bias-softmax\",action=\"store_true\")\n    parser.add_argument(\"--epochs\",      type=int,   default=20)\n    parser.add_argument(\"--batch-size\",  type=int,   default=16)\n    parser.add_argument(\"--lr\",          type=float, default=1e-4)\n    parser.add_argument(\"--patience\",    type=int,   default=5)\n    parser.add_argument(\"--data-dir\",    type=str,   default=\"/kaggle/working/data\")\n    args = parser.parse_args()\n\n    run_name = (\n        f\"{args.backbone}_SZ{args.image_size}\"\n        + f\"_PR{'Y' if args.progressive else 'N'}\"\n        + f\"_BAS{'Y' if args.bias_softmax else 'N'}\"\n    )\n\n    results = run_configuration(\n        backbone_name=args.backbone,\n        image_size=args.image_size,\n        use_progressive=args.progressive,\n        use_bas=args.bias_softmax,\n        data_dir=args.data_dir,\n        total_epochs=args.epochs,\n        batch_size=args.batch_size,\n        base_lr=args.lr,\n        weight_decay=1e-4,\n        early_stopping_patience=args.patience\n    )\n\n    os.makedirs(\"outputs/curves\", exist_ok=True)\n    plot_metrics_per_model(results, run_name, outdir=\"outputs/curves\")\n\n    os.makedirs(\"outputs/results\", exist_ok=True)\n    csv_path = \"outputs/results/prnet_results.csv\"\n    df = pd.read_csv(csv_path) if os.path.exists(csv_path) else pd.DataFrame()\n    row = {k: v for k, v in results.items() if not isinstance(v, list)}\n    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n    # df = df.append(row, ignore_index=True)\n    df.to_csv(csv_path, index=False)\n    # â†’ Test Acc = {results['Test Accuracy']:.4f}\n    print(f\"âœ… Done {run_name} \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:51:54.694744Z","iopub.execute_input":"2025-07-22T07:51:54.695351Z","iopub.status.idle":"2025-07-22T07:51:54.711207Z","shell.execute_reply.started":"2025-07-22T07:51:54.695328Z","shell.execute_reply":"2025-07-22T07:51:54.710662Z"}},"outputs":[{"name":"stdout","text":"Overwriting train.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!ls -l /kaggle/working/train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:51:58.902875Z","iopub.execute_input":"2025-07-22T07:51:58.903406Z","iopub.status.idle":"2025-07-22T07:51:59.024263Z","shell.execute_reply.started":"2025-07-22T07:51:58.903384Z","shell.execute_reply":"2025-07-22T07:51:59.023574Z"}},"outputs":[{"name":"stdout","text":"-rw-r--r-- 1 root root 28097 Jul 22 07:51 /kaggle/working/train.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"%%bash\nPYTHONUNBUFFERED=1 python -u /kaggle/working/train.py \\\n  --backbone efficientnet_b5 \\\n  --image-size 512 \\\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:52:20.539465Z","iopub.execute_input":"2025-07-22T07:52:20.539753Z","iopub.status.idle":"2025-07-22T12:20:51.038949Z","shell.execute_reply.started":"2025-07-22T07:52:20.539726Z","shell.execute_reply":"2025-07-22T12:20:51.038143Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nPath to dataset files: /kaggle/input/covid19-radiography-database\n\nInspecting folder: /kaggle/input/covid19-radiography-database\nSubdirectories: ['COVID-19_Radiography_Dataset']\n\nInspecting folder: /kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset\nSubdirectories: ['Normal', 'Lung_Opacity', 'Viral Pneumonia', 'COVID']\n\nInspecting folder: /kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/Normal\nSubdirectories: ['images', 'masks']\n\nInspecting folder: /kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/Normal/images\nSubdirectories: []\n\nInspecting folder: /kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/Normal/masks\nSubdirectories: []\n\nInspecting folder: /kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/Lung_Opacity\nSubdirectories: ['images', 'masks']\n\nInspecting folder: /kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/Lung_Opacity/images\nSubdirectories: []\n\nInspecting folder: /kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/Lung_Opacity/masks\nSubdirectories: []\n\nInspecting folder: /kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/Viral Pneumonia\nSubdirectories: ['images', 'masks']\n\nInspecting folder: /kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/Viral Pneumonia/images\nSubdirectories: []\n\nInspecting folder: /kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/Viral Pneumonia/masks\nSubdirectories: []\n\nInspecting folder: /kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/COVID\nSubdirectories: ['images', 'masks']\n\nInspecting folder: /kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/COVID/images\nSubdirectories: []\n\nInspecting folder: /kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/COVID/masks\nSubdirectories: []\nstarting stage 1/1: resolution 512x512, batch size 8\n                                                                                                     \nEpoch 1: train_acc=0.2710, val_acc=0.2407, train_loss=2.1874, val_loss=1.7880\nnew best val acc: 0.2407 - saved to efficientnet_b5_stage1_512.pth\nnew overall best train acc: 0.2710\nnew overall best val acc: 0.2407\n                                                                                                     \nEpoch 2: train_acc=0.8530, val_acc=0.9485, train_loss=0.4394, val_loss=0.1532\nnew best val acc: 0.9485 - saved to efficientnet_b5_stage1_512.pth\nnew overall best train acc: 0.8530\nnew overall best val acc: 0.9485\n                                                                                                     \nEpoch 3: train_acc=0.9048, val_acc=0.9305, train_loss=0.2886, val_loss=0.1721\nno improvement for 1 epochs\n                                                                                                     \nEpoch 4: train_acc=0.9119, val_acc=0.9546, train_loss=0.2524, val_loss=0.1468\nnew best val acc: 0.9546 - saved to efficientnet_b5_stage1_512.pth\nnew overall best train acc: 0.9119\nnew overall best val acc: 0.9546\n                                                                                                     \nEpoch 5: train_acc=0.9289, val_acc=0.9480, train_loss=0.2159, val_loss=0.1572\nno improvement for 1 epochs\n                                                                                                     \nEpoch 6: train_acc=0.9328, val_acc=0.9560, train_loss=0.2008, val_loss=0.1381\nnew best val acc: 0.9560 - saved to efficientnet_b5_stage1_512.pth\nnew overall best train acc: 0.9328\nnew overall best val acc: 0.9560\n                                                                                                     \nEpoch 7: train_acc=0.9395, val_acc=0.9574, train_loss=0.1736, val_loss=0.1150\nnew best val acc: 0.9574 - saved to efficientnet_b5_stage1_512.pth\nnew overall best train acc: 0.9395\nnew overall best val acc: 0.9574\n                                                                                                     \nEpoch 8: train_acc=0.9475, val_acc=0.9187, train_loss=0.1590, val_loss=0.2287\nno improvement for 1 epochs\n                                                                                                     \nEpoch 9: train_acc=0.9541, val_acc=0.9527, train_loss=0.1345, val_loss=0.1536\nno improvement for 2 epochs\n                                                                                                      \nEpoch 10: train_acc=0.9587, val_acc=0.9556, train_loss=0.1195, val_loss=0.1481\nno improvement for 3 epochs\n                                                                                                      \nEpoch 11: train_acc=0.9679, val_acc=0.9589, train_loss=0.0950, val_loss=0.1194\nnew best val acc: 0.9589 - saved to efficientnet_b5_stage1_512.pth\nnew overall best train acc: 0.9679\nnew overall best val acc: 0.9589\n                                                                                                      \nEpoch 12: train_acc=0.9709, val_acc=0.9645, train_loss=0.0853, val_loss=0.1276\nnew best val acc: 0.9645 - saved to efficientnet_b5_stage1_512.pth\nnew overall best train acc: 0.9709\nnew overall best val acc: 0.9645\n                                                                                                      \nEpoch 13: train_acc=0.9784, val_acc=0.9645, train_loss=0.0653, val_loss=0.1091\nno improvement for 1 epochs\n                                                                                                      \nEpoch 14: train_acc=0.9828, val_acc=0.9674, train_loss=0.0491, val_loss=0.1191\nnew best val acc: 0.9674 - saved to efficientnet_b5_stage1_512.pth\nnew overall best train acc: 0.9828\nnew overall best val acc: 0.9674\n                                                                                                      \nEpoch 15: train_acc=0.9886, val_acc=0.9678, train_loss=0.0326, val_loss=0.1255\nnew best val acc: 0.9678 - saved to efficientnet_b5_stage1_512.pth\nnew overall best train acc: 0.9886\nnew overall best val acc: 0.9678\n                                                                                                      \nEpoch 16: train_acc=0.9914, val_acc=0.9660, train_loss=0.0267, val_loss=0.1547\nno improvement for 1 epochs\n                                                                                                      \nEpoch 17: train_acc=0.9929, val_acc=0.9660, train_loss=0.0204, val_loss=0.1421\nno improvement for 2 epochs\n                                                                                                      \nEpoch 18: train_acc=0.9951, val_acc=0.9626, train_loss=0.0151, val_loss=0.1446\nno improvement for 3 epochs\n                                                                                                      \nEpoch 19: train_acc=0.9966, val_acc=0.9622, train_loss=0.0127, val_loss=0.1519\nno improvement for 4 epochs\n                                                                                                      \nEpoch 20: train_acc=0.9965, val_acc=0.9655, train_loss=0.0101, val_loss=0.1474\nno improvement for 5 epochs\nearly stopping triggered at stage 1, epoch 20\n\nearly stopping triggered - stopping all stages\nâœ… loaded best validation model from efficientnet_b5_stage1_512.pth (val_acc=0.9678)\nrunning final test evaluation...\nFinal test evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:57<00:00,  2.32it/s]\n\nfinal results:\n  best train acc (across all stages): 0.9886\n  best val acc (across all stages): 0.9678\n  final test acc: 0.9608\nSaved plot as outputs/curves/efficientnet_b5_SZ512_PRN_BASN.png\n/kaggle/working/outputs/curves/efficientnet_b5_SZ512_PRN_BASN.png\nFigure(1200x400)\nâœ… Done efficientnet_b5_SZ512_PRN_BASN \n","output_type":"stream"},{"name":"stderr","text":"train - COVID: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2892/2892 [00:03<00:00, 939.21it/s] \nval - COVID: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 361/361 [00:00<00:00, 905.49it/s] \ntest - COVID: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 363/363 [00:00<00:00, 907.37it/s] \ntrain - Normal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8153/8153 [00:12<00:00, 636.88it/s]\nval - Normal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1019/1019 [00:01<00:00, 725.54it/s]\ntest - Normal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1020/1020 [00:01<00:00, 692.96it/s]\ntrain - Viral Pneumonia: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1076/1076 [00:01<00:00, 615.27it/s]\nval - Viral Pneumonia: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 134/134 [00:00<00:00, 605.21it/s]\ntest - Viral Pneumonia: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:00<00:00, 637.21it/s]\ntrain - Lung_Opacity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4809/4809 [00:07<00:00, 626.13it/s]\nval - Lung_Opacity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 601/601 [00:01<00:00, 573.57it/s]\ntest - Lung_Opacity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 602/602 [00:01<00:00, 597.78it/s]\n/kaggle/working/train.py:121: UserWarning: Argument(s) 'shift_limit' are not valid for transform OpticalDistortion\n  A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=0.3), # lens-like warping of image\n/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\n\noutput_path = '/kaggle/working/outputs/results/prnet_results.csv'\n\nif os.path.exists(output_path):\n    print(f\"âœ… Found file at {output_path}\")\nelse:\n    print(f\"âŒ No file at {output_path}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-22T14:39:04.926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(output_path)\nprint(df)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-22T14:39:04.929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}